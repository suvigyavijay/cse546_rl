{"cells":[{"cell_type":"markdown","id":"9c0d943c","metadata":{"id":"9c0d943c"},"source":["## <center>CSE 546: Reinforcement Learning</center>\n","### <center>Prof. Alina Vereshchaka</center>\n","<!-- ### <center>Fall 2022</center> -->\n","\n","Welcome to the Assignment 2, Part 1: Introduction to Deep Reinforcement Learning and Neural Networks! The goal of this assignment is to make you comfortable with the application of different Neural Network structures depending on how the Reinforcement Learning environment is set up."]},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wDHNgRGMzYO","executionInfo":{"status":"ok","timestamp":1709771111064,"user_tz":300,"elapsed":20352,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}},"outputId":"33eea778-77b6-4420-9381-2646c12e084b"},"id":"_wDHNgRGMzYO","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","source":["# Imports\n","import cv2\n","import gymnasium as gym\n","from gymnasium import spaces\n","import matplotlib.pyplot as plt\n","from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","import numpy as np\n","from time import time\n","\n","\n","# Defining the Wumpus World Environment.\n","class WumpusWorldEnvironment(gym.Env):\n","    \"\"\"This class implements the Wumpus World environment.\"\"\"\n","\n","    def __init__(self, observation_type, action_type):\n","        \"\"\"This method initializes the environment.\n","\n","        :param string observation_type: - It can take four values: 1. 'integer' 2. 'vector' 3. 'image' 4. 'float'\n","                                          determining the type of observation returned to the agent.\n","\n","        :param string action_type: It can take three values: 1. 'discrete' 2. 'continuous' 3. 'multi_discrete'\n","                                   determining the type of action the agent can take.\"\"\"\n","\n","        self.observation_type = observation_type.lower()\n","\n","        self.environment_width = 6\n","        self.environment_height = 6\n","\n","        self.observation_space = spaces.Discrete(self.environment_width * self.environment_height)\n","\n","        # Action.\n","        self.action_type = action_type.lower()\n","\n","        if self.action_type == 'discrete':\n","            self.action_space = spaces.Discrete(4)\n","        elif self.action_type == 'continuous':\n","            self.action_space = spaces.Box(low=np.array([-1]), high=np.array([1]))\n","        elif self.action_type == 'multi_discrete':\n","            self.action_space = spaces.MultiDiscrete([2, 2, 2, 2])\n","        else:\n","            raise Exception('Invalid action type. Valid action types are: '\n","                            '\\n1. discrete \\n2. continuous \\n3. multi_discrete')\n","\n","        # Positions of environment objects.\n","        self.agent_pos = np.asarray([0, 0])\n","\n","        self.breeze_pos = np.asarray([[1, 0], [3, 0], [5, 0], [2, 1], [4, 1], [1, 2], [3, 2], [5, 2], [0, 3],\n","                                      [2, 3], [1, 4], [3, 4], [5, 4], [0, 5], [2, 5], [4, 5]])\n","\n","        self.gold_pos = np.asarray([4, 5])\n","\n","        self.pit_pos = np.asarray([[2, 0], [5, 1], [2, 2], [0, 4], [2, 4], [3, 5], [5, 5]])\n","\n","        self.stench_pos = np.asarray([[3, 2], [2, 3], [4, 3], [3, 4]])\n","\n","        self.wumpus_pos = np.asarray([3, 3])\n","\n","        self.gold_quantity = 1\n","\n","        self.timesteps = 0\n","        self.max_timesteps = 1000\n","\n","        # Creating the mapping from the co-ordinates to integers representing the grid blocks.\n","        self.coordinates_state_mapping = {}\n","        for i in range(self.environment_height):\n","            for j in range(self.environment_width):\n","                self.coordinates_state_mapping[f'{np.asarray([j, i])}'] = i * self.environment_width + j\n","\n","        self.start_time = time()\n","\n","    def reset(self):\n","        \"\"\"This method resets the agent position and returns the state as the observation.\n","\n","        :returns observation: - Observation received by the agent (Type depends on the parameter observation_type).\"\"\"\n","\n","        self.agent_pos = np.asarray([0, 0])\n","\n","        observation = self.return_observation()\n","        self.timesteps = 0\n","        self.gold_quantity = 1\n","        info = {}\n","\n","        self.start_time = time()\n","\n","        return observation, info\n","\n","    def return_observation(self):\n","        \"\"\"This method returns the observation.\n","\n","        :returns observation - Observation received by the agent (Type depends on the parameter observation_type).\"\"\"\n","\n","        if self.observation_type == 'integer':\n","            observation = self.coordinates_state_mapping[f'{self.agent_pos}']\n","        elif self.observation_type == 'vector':\n","            observation = self.agent_pos\n","        elif self.observation_type == 'image':\n","            observation = self.render(plot=False)\n","        elif self.observation_type == 'float':\n","            time_elapsed = time() - self.start_time\n","            observation = time_elapsed\n","        else:\n","            raise Exception('Invalid observation type. Valid observation types are: '\n","                            '\\n1. integer \\n2. vector \\n3. image \\n4. float')\n","\n","        return observation\n","\n","    def take_action(self, action):\n","        \"\"\"This method takes the action.\n","\n","        :param action: - Action taken by the agent (Type depends on the parameter action_type).\"\"\"\n","\n","        if self.action_type == 'discrete':\n","            if action == 0:\n","                self.agent_pos[0] += 1  # Right.\n","            elif action == 1:\n","                self.agent_pos[0] -= 1  # Left.\n","            elif action == 2:\n","                self.agent_pos[1] += 1  # Up.\n","            elif action == 3:\n","                self.agent_pos[1] -= 1  # Down.\n","            else:\n","                raise Exception('InvalidAction: Discrete action can take values 0, 1, 2 or 3.')\n","\n","        elif self.action_type == 'continuous':\n","            if -1 <= action <= -0.5:\n","                self.agent_pos[0] += 1  # Right.\n","            elif -0.5 < action <= 0:\n","                self.agent_pos[0] -= 1  # Left.\n","            elif 0 < action <= 0.5:\n","                self.agent_pos[1] += 1  # Up.\n","            elif 0.5 < action <= 1:\n","                self.agent_pos[1] -= 1  # Down.\n","            else:\n","                raise Exception('InvalidAction: Continuous action has a range [-1, 1].')\n","\n","        elif self.action_type == 'multi_discrete':\n","            if action[0] == 1:\n","                self.agent_pos[0] += 1  # Right.\n","            if action[1] == 1:\n","                self.agent_pos[0] -= 1  # Left.\n","            if action[2] == 1:\n","                self.agent_pos[1] += 1  # Up.\n","            if action[3] == 1:\n","                self.agent_pos[1] -= 1  # Down.\n","            if len(action) != 4 or (action[0] not in [0, 1] or action[1] not in [0, 1] or action[2] not in [0, 1]\n","                                    or action[3] not in [0, 1]):\n","                raise Exception(\n","                    'InvalidAction: Multi-Discrete action takes binary values in the array [0, 0, 0, 0]. '\n","                    'Refer to the assignment problem statement on environment details.')\n","\n","    def step(self, action):\n","        \"\"\"This method implements what happens when the agent takes a particular action. It changes the agent's\n","        position (While not allowing it to go out of the environment space.), maps the environment co-ordinates to a\n","        state, defines the rewards for the various states, and determines when the episode ends.\n","\n","        :param action: - Action taken by the agent (Type depends on the parameter action_type).\n","\n","        :returns observation: - Observation received by the agent (Type depends on the parameter observation_type).\n","                 int reward: - Integer value that's used to measure the performance of the agent.\n","                 bool done: - Boolean describing whether the episode has ended.\n","                 dict info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n","\n","        self.take_action(action)\n","\n","        # Ensuring that the agent doesn't go out of the environment.\n","        self.agent_pos = np.clip(self.agent_pos, a_min=[0, 0],\n","                                 a_max=[self.environment_width - 1, self.environment_height - 1])\n","\n","        observation = self.return_observation()\n","\n","        reward, terminated, truncated = None, None, None\n","        info = {}\n","\n","        return observation, reward, terminated, truncated, info\n","\n","    def render(self, mode='human', plot=False):\n","        \"\"\"This method renders the environment.\n","\n","        :param string mode: 'human' renders to the current display or terminal and returns nothing.\n","\n","        :param boolean plot: Boolean indicating whether we show a plot or not.\n","\n","                             If False, the method returns a resized NumPy array representation of the environment\n","                             to be used as the state.\n","\n","                             If True it plots the environment.\n","\n","        :returns array preprocessed_image: Grayscale NumPy array representation of the environment.\"\"\"\n","\n","        fig, ax = plt.subplots(figsize=(15, 15))\n","        ax.set_xlim(0, 6)\n","        ax.set_ylim(0, 6)\n","\n","        def plot_image(plot_pos):\n","            \"\"\"This is a helper function to render the environment. It checks which objects are in a particular\n","            position on the grid and renders the appropriate image.\n","\n","            :param arr plot_pos: Co-ordinates of the grid position which needs to be rendered.\"\"\"\n","\n","            # Initially setting every object to not be plotted.\n","            plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus = \\\n","                False, False, False, False, False, False\n","\n","            # Checking which objects need to be plotted by comparing their positions.\n","            if np.array_equal(self.agent_pos, plot_pos):\n","                plot_agent = True\n","            if any(np.array_equal(self.breeze_pos[i], plot_pos) for i in range(len(self.breeze_pos))):\n","                plot_breeze = True\n","            if self.gold_quantity > 0:  # Gold isn't plotted if it has already been picked by one of the agents.\n","                if np.array_equal(plot_pos, self.gold_pos):\n","                    plot_gold = True\n","            if any(np.array_equal(self.pit_pos[i], plot_pos) for i in range(len(self.pit_pos))):\n","                plot_pit = True\n","            if any(np.array_equal(self.stench_pos[i], plot_pos) for i in range(len(self.stench_pos))):\n","                plot_stench = True\n","            if np.array_equal(plot_pos, self.wumpus_pos):\n","                plot_wumpus = True\n","\n","            # Plot for Agent.\n","            if plot_agent and \\\n","                    all(not item for item in\n","                        [plot_breeze, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n","                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent.png'), zoom=0.28),\n","                                       np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(agent)\n","\n","            # Plot for Breeze.\n","            elif plot_breeze and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_gold, plot_pit, plot_stench, plot_wumpus]):\n","                breeze = AnnotationBbox(OffsetImage(plt.imread('./images/breeze.png'), zoom=0.28),\n","                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(breeze)\n","\n","            # Plot for Gold.\n","            elif plot_gold and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_breeze, plot_pit, plot_stench, plot_wumpus]):\n","                gold = AnnotationBbox(OffsetImage(plt.imread('./images/gold.png'), zoom=0.28),\n","                                      np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(gold)\n","\n","            # Plot for Pit.\n","            elif plot_pit and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_breeze, plot_gold, plot_stench, plot_wumpus]):\n","                pit = AnnotationBbox(OffsetImage(plt.imread('./images/pit.png'), zoom=0.28),\n","                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(pit)\n","\n","            # Plot for Stench.\n","            elif plot_stench and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_wumpus]):\n","                stench = AnnotationBbox(OffsetImage(plt.imread('./images/stench.png'), zoom=0.28),\n","                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(stench)\n","\n","            # Plot for Wumpus.\n","            elif plot_wumpus and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_breeze, plot_gold, plot_pit, plot_stench]):\n","                wumpus = AnnotationBbox(OffsetImage(plt.imread('./images/wumpus.png'), zoom=0.28),\n","                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(wumpus)\n","\n","            # Plot for Agent and Breeze.\n","            elif all(item for item in [plot_agent, plot_breeze]) and \\\n","                    all(not item for item in\n","                        [plot_gold, plot_pit, plot_stench, plot_wumpus]):\n","                agent_breeze = AnnotationBbox(OffsetImage(plt.imread('./images/agent_breeze.png'), zoom=0.28),\n","                                              np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(agent_breeze)\n","\n","            # Plot for Agent and Pit.\n","            elif all(item for item in [plot_agent, plot_pit]) and \\\n","                    all(not item for item in\n","                        [plot_breeze, plot_gold, plot_stench, plot_wumpus]):\n","                agent_pit = AnnotationBbox(OffsetImage(plt.imread('./images/agent_dead_pit.png'), zoom=0.28),\n","                                           np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(agent_pit)\n","\n","            # Plot for Agent and Stench.\n","            elif all(item for item in [plot_agent, plot_stench]) and \\\n","                    all(not item for item in\n","                        [plot_breeze, plot_gold, plot_pit, plot_wumpus]):\n","                agent_stench = AnnotationBbox(OffsetImage(plt.imread('./images/agent_stench.png'), zoom=0.28),\n","                                              np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(agent_stench)\n","\n","            # Plot for Agent, Breeze and Stench.\n","            elif all(item for item in [plot_agent, plot_breeze, plot_stench]) and \\\n","                    all(not item for item in\n","                        [plot_gold, plot_pit, plot_wumpus]):\n","                agent_breeze_stench = AnnotationBbox(OffsetImage(plt.imread('./images/agent_breeze_stench.png'),\n","                                                                 zoom=0.28), np.add(plot_pos, [0.5, 0.5]),\n","                                                     frameon=False)\n","                ax.add_artist(agent_breeze_stench)\n","\n","            # Plot for Agent and Wumpus.\n","            elif all(item for item in [plot_agent, plot_wumpus]) and \\\n","                    all(not item for item in\n","                        [plot_gold, plot_pit, plot_stench, plot_breeze]):\n","                agent_wumpus = AnnotationBbox(OffsetImage(plt.imread('./images/agent_dead_wumpus_alive.png'),\n","                                                          zoom=0.28), np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(agent_wumpus)\n","\n","            # Plot for Breeze and Gold.\n","            elif all(item for item in [plot_breeze, plot_gold]) and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_pit, plot_stench, plot_wumpus]):\n","                breeze_gold = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_gold.png'), zoom=0.28),\n","                                             np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(breeze_gold)\n","\n","            # Plot for Breeze and Stench.\n","            elif all(item for item in [plot_breeze, plot_stench]) and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_gold, plot_pit, plot_wumpus]):\n","                breeze_stench = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_stench.png'), zoom=0.28),\n","                                               np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(breeze_stench)\n","\n","            # Plot for Breeze, Stench, and Gold.\n","            elif all(item for item in [plot_breeze, plot_gold, plot_stench]) and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_pit, plot_wumpus]):\n","                breeze_gold_stench = AnnotationBbox(OffsetImage(plt.imread('./images/breeze_gold_stench.png'),\n","                                                                zoom=0.28), np.add(plot_pos, [0.5, 0.5]),\n","                                                    frameon=False)\n","                ax.add_artist(breeze_gold_stench)\n","\n","            # Plot for Stench and Gold.\n","            elif all(item for item in [plot_stench, plot_gold]) and \\\n","                    all(not item for item in\n","                        [plot_agent, plot_breeze, plot_pit, plot_wumpus]):\n","                stench_gold = AnnotationBbox(OffsetImage(plt.imread('./images/stench_gold.png'), zoom=0.28),\n","                                             np.add(plot_pos, [0.5, 0.5]), frameon=False)\n","                ax.add_artist(stench_gold)\n","\n","        coordinates_state_mapping_2 = {}\n","        for j in range(self.environment_height * self.environment_width):\n","            coordinates_state_mapping_2[j] = np.asarray(\n","                [j % self.environment_width, int(np.floor(j / self.environment_width))])\n","\n","        # Rendering the images for all states.\n","        for position in coordinates_state_mapping_2:\n","            plot_image(coordinates_state_mapping_2[position])\n","\n","        plt.xticks([0, 1, 2, 3, 4, 5])\n","        plt.yticks([0, 1, 2, 3, 4, 5])\n","        plt.grid()\n","\n","        if plot:  # Displaying the plot.\n","            plt.show()\n","        else:  # Returning the preprocessed image representation of the environment.\n","            fig.canvas.draw()\n","            img = np.array(fig.canvas.renderer.buffer_rgba())[:, :, :1]\n","            width = int(84)\n","            height = int(84)\n","            dim = (width, height)\n","            # noinspection PyUnresolvedReferences\n","            preprocessed_image = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n","            plt.close(fig)\n","            return preprocessed_image"],"metadata":{"id":"pXE38tInMofW","executionInfo":{"status":"ok","timestamp":1709771112091,"user_tz":300,"elapsed":1029,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"id":"pXE38tInMofW","execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"id":"c0fc19e1","metadata":{"id":"c0fc19e1","executionInfo":{"status":"ok","timestamp":1709771112091,"user_tz":300,"elapsed":3,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"outputs":[],"source":["# # Imports\n","# from environment import WumpusWorldEnvironment"]},{"cell_type":"markdown","id":"87890afd","metadata":{"id":"87890afd"},"source":["# Environment"]},{"cell_type":"markdown","id":"b9dc8362","metadata":{"id":"b9dc8362"},"source":["We will be working with an implementation of the Wumpus World environment. The environment comes from the book \"Artificial Intelligence: A Modern Approach\" by Stuart J. Russell and Peter Norvig.\n","\n","### ENVIRONMENT DETAILS:\n","\n","The environment is a 6 x 6 grid world containing a total of 36 grid blocks.\n","\n","#### ENVIRONMENT OBJECTS:\n","The environment consists of the following objects:\n","\n","1. **Agent** - The agent starts in the grid block at the bottom left corner whose co-ordinates are [0, 0]. The goal of our agent is to collect the Gold while avoiding the Wumpus and the pits.\n","\n","2. **Wumpus** - The monster which would eat the agent if they are in the same grid block.\n","\n","3. **Pit** - The agent must avoid falling into the pits.\n","\n","4. **Gold** - The agent must collect the Gold.\n","\n","5. **Breeze** - Breeze surrounds the Pits and warn the agent of a Pit in an adjacent grid block.\n","\n","6. **Stench** - Stench surrounds the Wumpus and warns the agent of the Wumpus in an adjacent grid block.\n","\n","#### ENVIRONMENT OBSERVATIONS:\n","\n","Our implementation of the environment provides you with four different types of observations:\n","\n","1. **Integer** - Integer in the range [0 - 35]. This represents the grid block the agent is in. E.g., if the agent is in the bottom left grid block (starting position) the observation would be 0, if the agent is in the grid block containing the Gold the observation would be 34, if the agent is in the top right grid block the observation would be 35.\n","\n","2. **Vector** -\n","\n","    **2.1.** A vector of length 2 representing the agent co-ordinates. The first entry represents the x co-ordinate and the second entry represets the y co-ordinate. E.g., if the agent is in the bottom left grid block (starting position) the observation would be [0, 0], if the agent is in the grid block containing the Gold the observation would be [4, 5], if the agent is in the top right grid block the observation would be [5, 5].\n","    \n","    **2.2.** A vector of length 36 representing the one-hot encoding of the integer observation (refer type 1 above). E.g., if the agent is in the bottom left grid block (starting position) the observation would be [1, 0, ..., 0, 0], if the agent is in the grid block containing the Gold the observation would be [0, 0, ..., 1, 0], if the agent is in the top right grid block the observation would be [0, 0, ..., 0, 1].\n","\n","\n","3. **Image** - Image render of the environment returned as an NumPy array. The image size is 84 * 84 (same size used in the DQN paper). E.g., if the agent is in the bottom right grid block the observation is:\n","\n","    Observation: (84 * 84)\n","\n","     [[255 255 255 ... 255 255 255]\n","\n","     [255 255 255 ... 255 255 255]\n","\n","     [255 255 255 ... 255 255 255]\n","\n","     ...\n","\n","     [255 255 255 ... 255 255 255]\n","\n","     [255 255 255 ... 255 255 255]\n","\n","     [255 255 255 ... 255 255 255]]\n","\n","    Observation type: <class 'numpy.ndarray'>\n","\n","    Observation Shape: (84, 84)\n","\n","    Visually, it looks like:\n","    <img src=\"./images/environment_render.png\" width=\"500\" height=\"500\">\n","    \n","\n","4. **Float** - Float in the range [0 - $\\infty$] representing the time elapsed in seconds.\n","\n","#### ENVIRONMENT ACTIONS:\n","\n","Our implementation of the environment provides you with three different types of actions:\n","\n","1. **Discrete** - Integer in the range [0 - 3] representing the four actions possible in the environment as follows: 0 - Right 1 - Left 2 - Up 3 - Down.\n","\n","2. **Multi-Discrete** - Array of length four where each element takes binary values 0 or 1. Array elements represent if we take a particular action. Array element with index 0 corresponds to the right action, index 1 corresponds to the left action, index 2 corresponds to the up action, and index 3 corresponds to the down action. E.g.,\n","   action = [1, 0, 0, 0] would result in the agent moving right.\n","   action = [1, 0, 1, 0] would result in the agent moving right and up.\n","   action = [0, 1, 0, 1] would result in the agent moving left and down.\n","\n","3. **Continuous** - Float in the range [-1, 1] determining whether the agent will go left, right, up, or down as follows:\n","\n","    if -1 <= action <= -0.5:\n","        Go Right.\n","    elif -0.5 < action <= 0:\n","        Go Left.\n","    elif 0 < action <= 0.5:\n","        Go Up.\n","    elif 0.5 < action <= 1:\n","        Go Down.\n","        \n","### YOUR TASK IS TO USE A NEURAL NETWORK TO WORK WITH ALL FOUR TYPES OF OBSERVATIONS AND ALL THREE TYPES OF  ACTIONS.\n","### Note: You don't have to train your agent/neural network. You just have to build the neural network structure that takes the observation as input and produces the desired output with the initial weights.\n","\n","#### You can use libraries such as PyTorch/TensorFlow/Keras to build your neural networks.\n","\n","#### <span style=\"color:red\">You cannot use RL libraries that already provide the neural network to you such as Stable-baselines3, Keras-RL, TF agents, Ray RLLib etc.</span>"]},{"cell_type":"markdown","id":"a6deebbb","metadata":{"id":"a6deebbb"},"source":["<img src=\"./images/wumpus_world_environment.jpg\" width=\"600\" height=\"600\">"]},{"cell_type":"markdown","id":"f2f11e24","metadata":{"id":"f2f11e24"},"source":["# START COMPLETING YOUR ASSIGNMENT HERE"]},{"cell_type":"markdown","id":"729a8768","metadata":{"id":"729a8768"},"source":["## Observation Type - Integer, Action Type - Discrete\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is an integer (refer to environment observations type 1). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."]},{"cell_type":"markdown","id":"c596cb95","metadata":{"id":"c596cb95"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_1_64_4.png\">"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers"],"metadata":{"id":"7T2nlEtH_eA8","executionInfo":{"status":"ok","timestamp":1709771120625,"user_tz":300,"elapsed":8536,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"id":"7T2nlEtH_eA8","execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"id":"7b1f4da4","metadata":{"id":"7b1f4da4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709771121501,"user_tz":300,"elapsed":881,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}},"outputId":"d901eb44-7489-430d-ddc5-0d9327e8d048"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 64)                128       \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 260       \n","                                                                 \n","=================================================================\n","Total params: 388 (1.52 KB)\n","Trainable params: 388 (1.52 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","1/1 [==============================] - 0s 224ms/step\n","Observation: [[0]]\n","Predicted Q-values: [0. 0. 0. 0.]\n"]}],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='integer', action_type='discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","\n","model = tf.keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(1,)),\n","    layers.Dense(4, activation='linear')\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","observation = np.array([observation]).reshape(-1, 1)\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"b2b233bb","metadata":{"id":"b2b233bb"},"source":["## Observation Type - Vector (2.1), Action Type - Discrete\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 2 (refer to environment observations type 2.1). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."]},{"cell_type":"markdown","id":"0e9c4873","metadata":{"id":"0e9c4873"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_2_64_4.png\">"]},{"cell_type":"code","execution_count":6,"id":"10d985b5","metadata":{"id":"10d985b5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709771121860,"user_tz":300,"elapsed":366,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}},"outputId":"2f1760d2-42b7-4cbe-9458-ed69a0e7d935"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_2 (Dense)             (None, 64)                192       \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 260       \n","                                                                 \n","=================================================================\n","Total params: 452 (1.77 KB)\n","Trainable params: 452 (1.77 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","1/1 [==============================] - 0s 138ms/step\n","Observation: [[0 0]]\n","Predicted Q-values: [0. 0. 0. 0.]\n"]}],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='vector', action_type='discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","\n","model = tf.keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(2,)),\n","    layers.Dense(4, activation='linear')\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","observation = np.array([observation])\n","\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"a8957b9f","metadata":{"id":"a8957b9f"},"source":["## Observation Type - Vector (2.2), Action Type - Discrete\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1).\n","\n","**HINT:** Use the integer observation and convert it to a one-hot encoded vector."]},{"cell_type":"markdown","id":"470cf5f8","metadata":{"id":"470cf5f8"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_36_64_4.png\">"]},{"cell_type":"code","execution_count":7,"id":"ae05b09e","metadata":{"id":"ae05b09e","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1709771122540,"user_tz":300,"elapsed":681,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}},"outputId":"3428e79b-a879-4f8f-c9c4-6039a11c2a3a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'to_categorical' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-6ea3280141da>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWumpusWorldEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discrete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mone_hot_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# BEGIN_YOUR_CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"]}],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='integer', action_type='discrete')\n","observation, info = environment.reset()\n","one_hot_observation = to_categorical(observation, num_classes=36)\n","\n","# BEGIN_YOUR_CODE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","# Initialize the Sequential model\n","model = tf.keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=(36,)),\n","    layers.Dense(4, activation='linear')\n","])\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","\n","observation = np.array([one_hot_observation])\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"ce19c97b","metadata":{"id":"ce19c97b"},"source":["## Observation Type - Image, Action Type - Discrete\n","\n","The part of the assignment requires you to create a convolutional neural network with one convolutional layer having 128 filters of size 3 x 3, one hidden layer having 64 neurons, and the output layer having 4 neurons. The input to the neural network is an image of size 84 * 84 (refer to environment observations type 3). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."]},{"cell_type":"markdown","id":"df3d739c","metadata":{"id":"df3d739c"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/convolutional_neural_network_84x84_128_64_4.png\">"]},{"cell_type":"code","execution_count":null,"id":"2044da09","metadata":{"id":"2044da09","executionInfo":{"status":"aborted","timestamp":1709771122540,"user_tz":300,"elapsed":1,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"outputs":[],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='image', action_type='discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense\n","\n","model = tf.keras.Sequential([\n","    layers.Conv2D(128, (3, 3), activation='relu', input_shape=(84, 84, 1)),\n","    layers.Flatten(),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(4, activation='linear')\n","])\n","\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","observation = np.array([observation])\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"64be04fd","metadata":{"id":"64be04fd"},"source":["## Observation Type - Float, Action Type - Discrete\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 256 neurons and the output layer having 4 neurons. The input to the neural network is a float (refer to environment observations type 4). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."]},{"cell_type":"markdown","id":"b7ed49e0","metadata":{"id":"b7ed49e0"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_1_64_4.png\">"]},{"cell_type":"code","execution_count":null,"id":"ea0e4aa6","metadata":{"id":"ea0e4aa6","executionInfo":{"status":"aborted","timestamp":1709771122540,"user_tz":300,"elapsed":1,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"outputs":[],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='float', action_type='discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Initialize the Sequential model\n","model = tf.keras.Sequential([\n","    layers.Dense(256, activation='relu', input_shape=(1,)),\n","    layers.Dense(4, activation='linear')\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","observation = np.array([observation])\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"27040465","metadata":{"id":"27040465"},"source":["## Observation Type - Vector (2.2), Action Type - Multi-Discrete\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an array representing the probability of choosing the actions. (If the value of the array element is >=0.5 you will perform the action.) (refer to environment actions type 2).\n","\n","**HINT:** Use the integer observation and convert it to a one-hot encoded vector."]},{"cell_type":"markdown","id":"8fe64de9","metadata":{"id":"8fe64de9"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_36_64_4_sigmoid.png\">"]},{"cell_type":"code","execution_count":null,"id":"00ea3736","metadata":{"id":"00ea3736","executionInfo":{"status":"aborted","timestamp":1709771122540,"user_tz":300,"elapsed":1,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"outputs":[],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted action probabilities for the four actions. Print the observation and the action probabilities.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='integer', action_type='multi_discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","one_hot_input = to_categorical(observation, num_classes=36)\n","\n","model = Sequential([\n","    Dense(64, activation='relu', input_shape=(1,)),\n","    Dense(4, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy')\n","model.summary()\n","\n","observation = np.array([observation])\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","\n","# END_YOUR_CODE"]},{"cell_type":"markdown","id":"082f05b0","metadata":{"id":"082f05b0"},"source":["## Observation Type - Vector (2.2), Action Type - Continuous\n","\n","The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 1 neuron. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an float in the range [-1, 1] determining the action which will be taken. (refer to environment actions type 3).\n","\n","**HINT:** Use the integer observation and convert it to a one-hot encoded vector and use the TanH activation function to get the output in the range [-1, 1]."]},{"cell_type":"markdown","id":"d8796988","metadata":{"id":"d8796988"},"source":["The following figure shows the network structure you will have to use:\n","\n","<img src=\"./images/neural_network_structures/neural_network_36_64_1.png\">"]},{"cell_type":"code","execution_count":null,"id":"b98555f1","metadata":{"id":"b98555f1","executionInfo":{"status":"aborted","timestamp":1709771122540,"user_tz":300,"elapsed":1,"user":{"displayName":"Suvigya Vijay","userId":"09896464052443073582"}}},"outputs":[],"source":["\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n","and get the predicted action. Print the observation and the predicted action.\"\"\"\n","\n","environment = WumpusWorldEnvironment(observation_type='integer', action_type='multi_discrete')\n","observation, info = environment.reset()\n","\n","# BEGIN_YOUR_CODE\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","one_hot_observation = to_categorical(observation, num_classes=36)\n","\n","model = Sequential([\n","    Dense(64, activation='relu', input_shape=(36,)),\n","    Dense(1, activation='tanh')\n","])\n","\n","model.compile(optimizer='adam', loss='mse')\n","model.summary()\n","\n","\n","observation = np.array([one_hot_observation])\n","predicted_q_values = model.predict(observation)\n","\n","print(\"Observation:\", observation)\n","print(\"Predicted Q-values:\", predicted_q_values.flatten())\n","# END_YOUR_CODE"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}